{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALScdVidoC82"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "dataset = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Cleaning Text"
      ],
      "metadata": {
        "id": "tM0YAVDiKuM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentencess_train      = [i['context'] for i in tqdm(dataset['train'])]\n",
        "sentencess_validation = [i['context'] for i in tqdm(dataset['validation'])]\n",
        "\n",
        "sentencess = sentencess_train + sentencess_validation\n",
        "\n",
        "sents = []\n",
        "for sent in tqdm(sentencess):\n",
        "  sents += sent.split('.')\n",
        "\n",
        "del sentencess, sentencess_train, sentencess_validation\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    \"\"\"Removes special characters from text.\"\"\"\n",
        "    pattern = r\"[^a-zA-Z0-9\\s]\"  # Matches any character that is not alphanumeric or whitespace\n",
        "    cleaned_text = re.sub(pattern, \"\", text)\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_brackets(text):\n",
        "    \"\"\"Removes brackets from text.\"\"\"\n",
        "    pattern = r\"[\\(\\)\\[\\]\\{\\}]\"  # Matches any type of bracket\n",
        "    cleaned_text = re.sub(pattern, \"\", text)\n",
        "    return cleaned_text\n",
        "\n",
        "def lowercase_text(text):\n",
        "    \"\"\"Converts text to lowercase.\"\"\"\n",
        "    cleaned_text = text.lower()\n",
        "    return cleaned_text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Applies all cleaning functions to text.\"\"\"\n",
        "    text = remove_special_characters(text)\n",
        "    text = remove_brackets(text)\n",
        "    text = lowercase_text(text)\n",
        "    return text\n",
        "\n",
        "cleaned_text = [clean_text(text) for text in tqdm(sents)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4_LFFswoGei",
        "outputId": "14a4749d-698c-4003-a311-92a9c7b8ed39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [00:11<00:00, 7553.76it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:01<00:00, 5772.89it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98169/98169 [00:00<00:00, 283017.55it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 656224/656224 [00:05<00:00, 110273.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tokenization"
      ],
      "metadata": {
        "id": "K-r2kwZKKzS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_output = tokenizer(\n",
        "    cleaned_text,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=64,\n",
        "    return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "input_ids = tokenized_output[\"input_ids\"]\n",
        "attention_mask = tokenized_output[\"attention_mask\"]\n",
        "\n",
        "import torch\n",
        "\n",
        "input_ids = torch.tensor(input_ids)\n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "print(\"Tokenized Shape:\", input_ids.shape)  # (num_samples, max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVgnmobaoJNz",
        "outputId": "b8e93a9e-aad1-4e72-988b-ae08e81cd78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-421ca65df4e2>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids = torch.tensor(input_ids)\n",
            "<ipython-input-3-421ca65df4e2>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  attention_mask = torch.tensor(attention_mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Shape: torch.Size([656224, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Creating Features & Labels"
      ],
      "metadata": {
        "id": "8zE1ZoqcK68A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = input_ids.clone()\n",
        "\n",
        "rand = torch.rand(input_ids.shape)\n",
        "mask_arr = (rand < 0.15) * (input_ids != tokenizer.pad_token_id) * (input_ids != tokenizer.cls_token_id)\n",
        "\n",
        "input_ids[mask_arr] = tokenizer.mask_token_id\n",
        "\n",
        "print(\"Original Sentence:\", tokenizer.convert_ids_to_tokens(labels[0].tolist()))\n",
        "print(\"Masked Sentence:\", tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpdhOXvOoNML",
        "outputId": "695f54ec-41fd-4ab8-a041-16187cd4c45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ['[CLS]', 'architectural', '##ly', 'the', 'school', 'has', 'a', 'catholic', 'character', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "Masked Sentence: ['[CLS]', 'architectural', '[MASK]', 'the', 'school', 'has', 'a', 'catholic', '[MASK]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Creating Data Generator"
      ],
      "metadata": {
        "id": "d-oG0Hx0K_vU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "print(f\"Total batches: {len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjWxlkGjoNr3",
        "outputId": "5e91318b-e3f2-4048-f2ac-ba3915477d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total batches: 41014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Defining TinyBERT"
      ],
      "metadata": {
        "id": "AVnBtgqhLE3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertForMaskedLM\n",
        "\n",
        "# Define TinyBERT Configuration\n",
        "tiny_bert_config = BertConfig(\n",
        "    vocab_size=30522,           # Standard BERT vocabulary\n",
        "    hidden_size=128,             # Tiny hidden size\n",
        "    num_hidden_layers=2,         # Only 2 Transformer blocks\n",
        "    num_attention_heads=2,       # 2 attention heads\n",
        "    intermediate_size=512,       # Small feed-forward size\n",
        "    hidden_act=\"gelu\",           # Activation function\n",
        "    hidden_dropout_prob=0.1,     # Dropout for regularization\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    max_position_embeddings=64,  # Max sequence length\n",
        "    type_vocab_size=2,           # Sentence A/B embeddings\n",
        "    initializer_range=0.02        # Initialize weights\n",
        ")\n",
        "\n",
        "tiny_bert = BertForMaskedLM(config=tiny_bert_config)\n",
        "print(f\"TinyBERT Model Parameters: {tiny_bert.num_parameters()}\")  # ~4.4M parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fwrJTEzoQSJ",
        "outputId": "25e977de-0fa6-4348-e82a-3533fbc7b971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TinyBERT Model Parameters: 4359354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Train BERT"
      ],
      "metadata": {
        "id": "QE7kkKjRLOaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AdamW\n",
        "optimizer = AdamW(tiny_bert.parameters(), lr=5e-4)\n",
        "\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tiny_bert.to(device)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0  # Track total loss\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move data to GPU if available\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Forward pass (Masked Language Model)\n",
        "        outputs = tiny_bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass & optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: Loss = {total_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCWCuJyRoR8i",
        "outputId": "9c07eb45-c91b-45b4-c73d-8491cdc36878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41014/41014 [11:34<00:00, 59.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 0.3844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41014/41014 [11:39<00:00, 58.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss = 0.2864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41014/41014 [11:38<00:00, 58.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss = 0.2629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41014/41014 [11:40<00:00, 58.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss = 0.2509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41014/41014 [11:42<00:00, 58.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss = 0.2431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41014/41014 [11:41<00:00, 58.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Loss = 0.2374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41014/41014 [11:41<00:00, 58.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Loss = 0.2332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41014/41014 [11:36<00:00, 58.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Loss = 0.2298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41014/41014 [11:37<00:00, 58.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Loss = 0.2273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41014/41014 [11:39<00:00, 58.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Loss = 0.2253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Saving Model"
      ],
      "metadata": {
        "id": "9qFnu4cPLSSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_bert.save_pretrained(\"tiny_bert_trained\")\n",
        "tokenizer.save_pretrained(\"tiny_bert_trained\")\n",
        "print(\"TinyBERT Model Saved Successfully! ðŸŽ‰\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIkCdODWoYKn",
        "outputId": "0f018262-c72b-4f50-c7aa-0ede3dcc46ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TinyBERT Model Saved Successfully! ðŸŽ‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Defining Predictions Function"
      ],
      "metadata": {
        "id": "ZdqIkLITLVKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def predict_masked_word(text):\n",
        "    # Tokenize input and replace a word with [MASK]\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    masked_index = tokens.index(\"[MASK]\") if \"[MASK]\" in tokens else None\n",
        "\n",
        "    if masked_index is None:\n",
        "        raise ValueError(\"Text must contain '[MASK]' token for prediction.\")\n",
        "\n",
        "    # Convert tokens to input IDs\n",
        "    input_ids = tokenizer.encode(text, max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = tiny_bert(input_ids)\n",
        "        predictions = outputs.logits\n",
        "\n",
        "    # Get the predicted word (top 3 guesses)\n",
        "    predicted_token_ids = predictions[0, masked_index].topk(3).indices.tolist()\n",
        "    predicted_words = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
        "\n",
        "    return predicted_words"
      ],
      "metadata": {
        "id": "eFx7njmjohZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Mass Prediction Function Define"
      ],
      "metadata": {
        "id": "vmnsXbkMLcFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "tiny_bert.eval()\n",
        "\n",
        "def get_masked_sentence(cleaned_text):\n",
        "    # Select a valid random sentence (must be at least 3 words long)\n",
        "    while True:\n",
        "        sentence = random.choice(cleaned_text)\n",
        "        words = sentence.split()\n",
        "\n",
        "        if len(words) > 2:  # Ensure at least 3 words for masking\n",
        "            break\n",
        "\n",
        "    # Choose a random position to mask (excluding first and last words)\n",
        "    mask_index = random.randint(1, len(words) - 2)\n",
        "\n",
        "    # Replace selected word with [MASK]\n",
        "    words[mask_index] = \"[MASK]\"\n",
        "\n",
        "    # Reconstruct masked sentence\n",
        "    masked_sentence = \" \".join(words)\n",
        "\n",
        "    return masked_sentence"
      ],
      "metadata": {
        "id": "QVBkR7S_oj03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Getting bulk Predictions"
      ],
      "metadata": {
        "id": "IvqCiknLLfaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "  masked_sentence = get_masked_sentence(cleaned_text)\n",
        "\n",
        "  predicted_words = predict_masked_word(masked_sentence)\n",
        "\n",
        "  print(f\"Masked Sentence: {masked_sentence}\")\n",
        "  print(f\"Predicted Words: {predicted_words}\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7oyenb0opd2",
        "outputId": "24fba91d-cd97-4f9f-d6ce-a413100f893f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masked Sentence: the television network has eight ownedandoperated and over 232 [MASK] television stations throughout the united states and its territories\n",
            "Predicted Words: ['232', '230', '375']\n",
            "\n",
            "Masked Sentence: scholars generally date these texts to around the 3rd century bce 100 to 200 years after the death of [MASK] buddha\n",
            "Predicted Words: ['of', 'to', 'for']\n",
            "\n",
            "Masked Sentence: the dissolution of the soviet union was formally enacted on december 26 [MASK] as a result of the declaration no\n",
            "Predicted Words: ['26', '11', '17']\n",
            "\n",
            "Masked Sentence: the foundation has since [MASK] to work with other charities in the city and also provided relief following hurricane ike three years later\n",
            "Predicted Words: ['since', 'until', 'before']\n",
            "\n",
            "Masked Sentence: this strategy was to a degree forced upon france geography coupled with the superiority of the british navy made it difficult for the french navy [MASK] provide significant supplies and support to french colonies\n",
            "Predicted Words: ['navy', 'army', 'force']\n",
            "\n",
            "Masked Sentence: the move reduced beyoncs familys income by half [MASK] her parents were forced to move into separated apartments\n",
            "Predicted Words: ['half', 'part', 'polynomial']\n",
            "\n",
            "Masked Sentence: nagumo executed a first strike against [MASK] while fletcher launched his aircraft bound for nagumos carriers\n",
            "Predicted Words: ['against', 'for', 'by']\n",
            "\n",
            "Masked Sentence: citation needed near his [MASK] in qishlak afshona some 25 km 16 mi north of bukhara a training college for medical staff has been named for him\n",
            "Predicted Words: ['his', 'their', 'its']\n",
            "\n",
            "Masked Sentence: in 1775 patrick henry delivered [MASK] famous give me liberty or give me death speech in st\n",
            "Predicted Words: ['delivered', 'presented', 'sealed']\n",
            "\n",
            "Masked Sentence: in the [MASK] 1880s kanehiro takaki observed that japanese sailors whose diets consisted almost entirely of white rice developed beriberi or endemic neuritis a disease causing heart problems and paralysis but british sailors and japanese naval officers did not\n",
            "Predicted Words: ['the', 'his', 'these']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Zip the Model to Download"
      ],
      "metadata": {
        "id": "XbKHQjuxLp-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/tiny_bert_trained.zip /content/tiny_bert_trained"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s28bMCEHjXq",
        "outputId": "b104e9f9-bbac-454a-b928-b7036f63d4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/tiny_bert_trained/ (stored 0%)\n",
            "  adding: content/tiny_bert_trained/config.json (deflated 48%)\n",
            "  adding: content/tiny_bert_trained/model.safetensors (deflated 9%)\n",
            "  adding: content/tiny_bert_trained/vocab.txt (deflated 53%)\n",
            "  adding: content/tiny_bert_trained/tokenizer_config.json (deflated 75%)\n",
            "  adding: content/tiny_bert_trained/generation_config.json (deflated 8%)\n",
            "  adding: content/tiny_bert_trained/special_tokens_map.json (deflated 42%)\n"
          ]
        }
      ]
    }
  ]
}